
# **File Upload & Scrape Process System - README**

## **Project Overview**
This project provides a system to allow users to upload Excel files containing Amazon ASINs, scrape product details (such as manufacturer name, address, and email), and return an updated Excel file with the scraped data. The system uses **Puppeteer** for web scraping, **Express.js** for the server, and **Multer** for file upload handling. Users can upload files through a simple web interface and download the processed file once the scraping and data processing are completed.

---

## **Features**
- **File Upload**: Users can upload Excel files containing Amazon ASINs.
- **Scraping**: Scrape product details from Amazon using Puppeteer.
- **File Processing**: Update the Excel file with the scraped data.
- **Downloadable Processed Files**: Once the processing is complete, users can download the updated file.
- **Real-time Feedback**: A progress loader is displayed while the file is being processed.
- **Validation**: Ensures that the file is of the correct type and does not exceed the size limit.

---

## **Technology Stack**
- **Node.js**: Backend server to handle API requests.
- **Express.js**: Framework used to build the API for file upload and processing.
- **Puppeteer**: Headless browser automation tool used to scrape Amazon product details.
- **Multer**: Middleware used for handling file uploads in Express.
- **xlsx**: A library to read from and write to Excel files.
- **Winston**: A logging library to track server activities and errors.
- **HTML, CSS, JavaScript**: Frontend for file upload and displaying progress.

---

## **Folder Structure**
```plaintext
project/
│
├── public/
│   └── index.html         # Frontend for uploading files
│
├── src/
│   ├── controllers/       # Logic for scraping and processing
│   │   └── scrapeController.js
│   ├── services/          # Services for handling Excel file reading and writing
│   │   └── excelService.js
│   ├── utils/             # Utility functions for logging and standardized responses
│   │   ├── logger.js
│   │   └── responses.js
│   ├── validators/        # File upload validation logic
│   │   └── uploadValidator.js
│   ├── config/            # Puppeteer configuration
│   │   └── puppeteer.config.js
│   └── app.js             # Main entry point for the Express app
├── uploads/               # Directory to store uploaded files
├── files/                 # Directory to store processed files
├── logs/                  # Log files generated by the app
│   └── app.log
└── package.json           # Node.js project dependencies and scripts
```

---

## **How It Works**

### **1. File Upload**

- **Frontend**: The user accesses a web interface where they can upload an Excel file containing ASINs.
- **Validation**: The uploaded file is validated:
  - File size should not exceed 10MB.
  - Only `.xlsx` file format is allowed.
- **Backend**: The backend receives the file and saves it in the `/uploads/` folder.
  
### **2. Scraping Process**

- **Puppeteer**: For each ASIN in the uploaded Excel file, Puppeteer launches a headless browser to navigate to the Amazon product page.
- **Scrape Data**: The required manufacturer details (name, address, email) are extracted from the page.
- **Update Excel**: The scraped data is added to the original data in the Excel file.

### **3. File Generation**

- The updated data is written to a new Excel file in the `/files/` directory with the same name prefixed by `updated-`.

### **4. File Download**

- Once the scraping and processing are complete, the user is provided with a link to download the updated file.

---

## **System Setup & Installation**

### **Prerequisites**
1. **Node.js**: Make sure you have Node.js installed. You can download it from [here](https://nodejs.org/).
2. **npm**: npm comes bundled with Node.js. Ensure you have it installed by running:
   ```bash
   npm -v
   ```

### **Steps to Run the Project**

1. **Clone the Repository**
   - Clone the repository to your local machine:
   ```bash
   git clone https://your-repository-url.git
   cd project-folder
   ```

2. **Install Dependencies**
   - Install all the required dependencies by running:
   ```bash
   npm install
   ```

3. **Run the Server**
   - Start the Express server:
   ```bash
   npm start
   ```
   - The server will start running on [http://localhost:3000](http://localhost:3000).

4. **Open the Web Interface**
   - Open your browser and visit [http://localhost:3000](http://localhost:3000) to upload the file.

---

## **API Endpoints**

1. **POST /upload**:
   - **Description**: Uploads an Excel file for processing.
   - **Request**: `multipart/form-data` with the file field named `file`.
   - **Response**: Returns the success message with the updated file's name or an error message if the upload fails.
   
   Example Response:
   ```json
   {
     "success": true,
     "message": "File processed successfully",
     "data": {
       "updatedFileName": "updated-asin-file.xlsx"
     }
   }
   ```

2. **GET /download/:fileName**:
   - **Description**: Allows the user to download the processed file.
   - **Request**: `GET` request with the file name as a URL parameter.
   - **Response**: The processed file will be downloaded.

---

## **Why Use Specific Code/Logic**

### **Why Puppeteer?**
Puppeteer is used for web scraping because it allows us to automate browser actions, such as opening a page, clicking elements, and extracting data, while handling complex modern web page layouts. It is perfect for scraping dynamic content that might be rendered with JavaScript.

### **Why Multer?**
Multer is a middleware for handling `multipart/form-data` in Express, which is essential for handling file uploads. It allows us to manage file uploads, validate them, and store them in a specified directory (`uploads/`).

### **Why `xlsx` Library?**
The `xlsx` library is used to read from and write to Excel files. It allows us to easily process Excel files and extract or insert data. We can convert Excel sheets to JSON and back to Excel files, which is ideal for our use case.

### **Why Use Logging (Winston)?**
Winston is a powerful logging library used to track and debug the application. It logs important events, such as file uploads, scraping activity, and errors, which helps to monitor the system's health and troubleshoot any issues that arise.

---

## **Common Issues & Troubleshooting**

1. **File Upload Fails**:
   - Ensure the file size does not exceed 10MB and that the file is in `.xlsx` format.
   - If there is an issue with the file upload, check the validation messages returned by the server.

2. **Scraping Issues**:
   - If the scraping fails (e.g., missing product details), make sure the Amazon ASINs are valid.
   - The Puppeteer browser might be blocked by CAPTCHA or other challenges on the website. Consider using proxies or headless browsing adjustments if necessary.

3. **File Download Not Working**:
   - Check that the processed file exists in the `/files/` directory. If not, check the server logs for errors related to file writing.

---

## **Conclusion**

This system automates the process of scraping product data from Amazon and updating an Excel file. It is built with modern web technologies to ensure smooth file upload, real-time feedback, and efficient scraping. The design is modular, separating concerns between file handling, scraping logic, and server-side validation, making it easier to extend and maintain.
